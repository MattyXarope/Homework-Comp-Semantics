{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 - Code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KEnSJ30oLXat",
        "X1kiZfEVK8Th",
        "4Bvpxa_PLLNM",
        "Bc89QxxkLQzV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattyXarope/Homework-Comp-Semantics/blob/main/Assignment_2_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEnSJ30oLXat"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGPxhD8PCrHm",
        "outputId": "3bb7c7ee-cc3d-4e5c-f53e-2c837ee924d2"
      },
      "source": [
        "# -*- coding: utf-8 -*- \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('word2vec_sample')\n",
        "from nltk.corpus import wordnet \n",
        "import pandas as pd\n",
        "import gensim\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import base64\n",
        "import requests\n",
        "import collections\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1kiZfEVK8Th"
      },
      "source": [
        "# Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grcHEvqOC0Mq",
        "outputId": "79028252-b5c0-4cfc-ec86-a70cc341f5a2"
      },
      "source": [
        "## Exercise 1 ##\n",
        "\n",
        "#Tests to see how many senses there are for the word language \n",
        "word = 'language'\n",
        "synsets = wordnet.synsets(word, lang=\"eng\")\n",
        "n_synsets = len(synsets)\n",
        "\n",
        "#Prints information about synsets for the word \n",
        "print(f'Lemma English: {word}')\n",
        "print(f'Number of synsets: {n_synsets}')\n",
        "print(f'Synsets details: ')\n",
        "for ss in synsets:\n",
        "    print(f'Name: {ss.name()}')\n",
        "    print(f'Gloss: {ss.definition()}')\n",
        "    print(f'Lemmas English: {ss.lemmas()}')\n",
        "    print(f'\\n')\n",
        "    \n",
        "#Establishes a concordance list for the word language in the Brown corpus (commented out due to use of sketch engine)\n",
        "# corpus = brown.words()\n",
        "# text = Text(corpus)\n",
        "# con_list = text.concordance_list(\"language\", width=250, lines=100)\n",
        "# df = pd.DataFrame(data={\"Utterance\": con_list})\n",
        "# df.to_csv(\"./Brown100.csv\", sep=',',index=False)\n",
        "\n",
        "#Adds file from SketchEngine of 100 instances of 'language' in the Brown corpus \n",
        "\n",
        "CSV_BROWN = \"https://raw.githubusercontent.com/MattyXarope/Homework-Comp-Semantics/main/Brown100Final.csv\"\n",
        "lang_syn = pd.read_csv(CSV_BROWN)\n",
        "\n",
        "#Create contigency table using data from lang_syn\n",
        "adelle = lang_syn['adelle']\n",
        "matt = lang_syn['matt']\n",
        "annotations = pd.DataFrame({'Adelle': adelle, 'Matt': matt})\n",
        "\n",
        "#Print contigency table data\n",
        "print(f'******************\\n')\n",
        "print('contingency table: ')\n",
        "print(f'******************\\n\\n')\n",
        "# note the 'margins=True' parameter -- make sure you understand what it does\n",
        "print(pd.crosstab(annotations['Adelle'], annotations['Matt'], margins=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma English: language\n",
            "Number of synsets: 6\n",
            "Synsets details: \n",
            "Name: language.n.01\n",
            "Gloss: a systematic means of communicating by the use of sounds or conventional symbols\n",
            "Lemmas English: [Lemma('language.n.01.language'), Lemma('language.n.01.linguistic_communication')]\n",
            "\n",
            "\n",
            "Name: speech.n.02\n",
            "Gloss: (language) communication by word of mouth\n",
            "Lemmas English: [Lemma('speech.n.02.speech'), Lemma('speech.n.02.speech_communication'), Lemma('speech.n.02.spoken_communication'), Lemma('speech.n.02.spoken_language'), Lemma('speech.n.02.language'), Lemma('speech.n.02.voice_communication'), Lemma('speech.n.02.oral_communication')]\n",
            "\n",
            "\n",
            "Name: lyric.n.01\n",
            "Gloss: the text of a popular song or musical-comedy number\n",
            "Lemmas English: [Lemma('lyric.n.01.lyric'), Lemma('lyric.n.01.words'), Lemma('lyric.n.01.language')]\n",
            "\n",
            "\n",
            "Name: linguistic_process.n.02\n",
            "Gloss: the cognitive processes involved in producing and understanding linguistic communication\n",
            "Lemmas English: [Lemma('linguistic_process.n.02.linguistic_process'), Lemma('linguistic_process.n.02.language')]\n",
            "\n",
            "\n",
            "Name: language.n.05\n",
            "Gloss: the mental faculty or power of vocal communication\n",
            "Lemmas English: [Lemma('language.n.05.language'), Lemma('language.n.05.speech')]\n",
            "\n",
            "\n",
            "Name: terminology.n.01\n",
            "Gloss: a system of words used to name things in a particular discipline\n",
            "Lemmas English: [Lemma('terminology.n.01.terminology'), Lemma('terminology.n.01.nomenclature'), Lemma('terminology.n.01.language')]\n",
            "\n",
            "\n",
            "******************\n",
            "\n",
            "contingency table: \n",
            "******************\n",
            "\n",
            "\n",
            "Matt    s1  s2  s3  s4  s5  s6  All\n",
            "Adelle                             \n",
            "s1      53  10   0   3   0   4   70\n",
            "s2       7   1   0   0   0   0    8\n",
            "s3       0   0   1   0   0   0    1\n",
            "s4       4   0   0   0   0   0    4\n",
            "s5       2   0   0   0   1   1    4\n",
            "s6       0   5   0   0   0   8   13\n",
            "All     66  16   1   3   1  13  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bvpxa_PLLNM"
      },
      "source": [
        "# Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhuJT2dCDJFp",
        "outputId": "753c9e4b-30b7-4c19-9c2a-005f54859c3b"
      },
      "source": [
        "## Exercise 2 ## \n",
        "\n",
        "#Does some calculations about the data from part 1\n",
        "\n",
        "#S1 Info\n",
        "#Gloss: a systematic means of communicating by the use of sounds or conventional symbols\n",
        "#Percent of all utterances \n",
        "all_percentS1 = (53/100)*100\n",
        "print(f'S1 was agreed upon by both annotators in {all_percentS1}% of all utterances')\n",
        "\n",
        "#S2 Info\n",
        "#Gloss: (language) communication by word of mouth\n",
        "#Percent of all utterances \n",
        "all_percentS2 = (1/100)*100\n",
        "print(f'S2 was agreed upon by both annotators in {all_percentS2}% of all utterances')\n",
        "\n",
        "#S3 Info\n",
        "#Gloss: the text of a popular song or musical-comedy number\n",
        "#Percent of all utterances \n",
        "all_percentS3 = (1/100)*100\n",
        "print(f'S3 was agreed upon by both annotators in {all_percentS3}% of all utterances')\n",
        "\n",
        "#S4 Info\n",
        "#Gloss: the cognitive processes involved in producing and understanding linguistic communication\n",
        "#Percent of all utterances \n",
        "all_percentS4 = (0/100)*100\n",
        "print(f'S4 was agreed upon by both annotators in {all_percentS4}% of all utterances')\n",
        "\n",
        "#S5 Info\n",
        "#Gloss: the mental faculty or power of vocal communication\n",
        "#Percent of all utterances \n",
        "all_percentS5 = (1/100)*100\n",
        "print(f'S5 was agreed upon by both annotators in {all_percentS5}% of all utterances')\n",
        "\n",
        "#S6 Info\n",
        "#Gloss: a system of words used to name things in a particular discipline\n",
        "#Percent of all utterances \n",
        "all_percentS6 = (8/100)*100\n",
        "print(f'S6 was agreed upon by both annotators in {all_percentS6}% of all utterances')\n",
        "\n",
        "#Print General Info for this section\n",
        "all_agreements = all_percentS1 + all_percentS2 + all_percentS3 + all_percentS4 + all_percentS5 + all_percentS6\n",
        "print(f'The annotators agreed on {all_agreements}% of all utterances')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S1 was agreed upon by both annotators in 53.0% of all utterances\n",
            "S2 was agreed upon by both annotators in 1.0% of all utterances\n",
            "S3 was agreed upon by both annotators in 1.0% of all utterances\n",
            "S4 was agreed upon by both annotators in 0.0% of all utterances\n",
            "S5 was agreed upon by both annotators in 1.0% of all utterances\n",
            "S6 was agreed upon by both annotators in 8.0% of all utterances\n",
            "The annotators agreed on 64.0% of all utterances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc89QxxkLQzV"
      },
      "source": [
        "# Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTwY0cc9DaY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c4beff-82d6-4888-877c-8b2736d1e441"
      },
      "source": [
        "## Exercise 3 ## \n",
        "\n",
        "#Sets path for word2vec models\n",
        "path_to_word2vec_sample = nltk.data.find('models/word2vec_sample/pruned.word2vec.txt')\n",
        "word2vec_gensim = gensim.models.KeyedVectors.load_word2vec_format(path_to_word2vec_sample)\n",
        "\n",
        "###Determines synonyms/antonyms###\n",
        "\n",
        "#Extracts the synonyms and hyponyms from wordnet\n",
        "lemmas=[]\n",
        "synonyms=[]\n",
        "antonyms=[]\n",
        "\n",
        "for word in wordnet.words(): \n",
        "    for syn in wordnet.synsets(word)[5:]: \n",
        "      for l in syn.lemmas():\n",
        "        if len(wordnet.synsets(word)) > 0 and len(l.antonyms()) > 0: \n",
        "          lemmas.append(syn.lemma_names()[0]) \n",
        "          synonyms.append(wordnet.synsets(word)[0].lemma_names()[0])\n",
        "          if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "#Fucntion to compare the cosine similarites in word2vec \n",
        "def cosine_gensim(list1, list2):\n",
        "  similarities = []\n",
        "  for elem1,elem2 in zip(list1,list2):\n",
        "    if elem1 in word2vec_gensim and elem2 in word2vec_gensim:\n",
        "      similarities.append(word2vec_gensim.similarity(elem1, elem2))  \n",
        "  return similarities\n",
        "\n",
        "\n",
        "#This just pairs them down to 500 data points\n",
        "synonyms500 = synonyms[:500]\n",
        "antonyms500 = antonyms[:500]\n",
        "\n",
        "#Gets the cosine similarity of the two groups \n",
        "syns_similar = cosine_gensim(lemmas, synonyms500)\n",
        "ants_similar = cosine_gensim(lemmas, antonyms500)\n",
        "\n",
        "\n",
        "###Determines hypo/hypernyms###\n",
        "\n",
        "#Extracts the hypernyms and hyponyms from wordnet \n",
        "lemmas2=[]\n",
        "hypernyms=[]\n",
        "hyponyms=[]\n",
        "\n",
        "for word in wordnet.words(): \n",
        "    for syn in wordnet.synsets(word)[5:]: \n",
        "      for l in syn.lemmas():\n",
        "        if len(syn.hypernyms()) > 0 and len(syn.hyponyms()) > 0: \n",
        "          lemmas2.append(syn.lemma_names()[0]) \n",
        "          hypernyms.append(syn.hypernyms()[0].lemma_names()[0])\n",
        "          hyponyms.append(syn.hyponyms()[0].lemma_names()[0])\n",
        "\n",
        "#This just pairs them down to 500 data points\n",
        "hypernyms500 = hypernyms[:500]\n",
        "hyponyms500 = hyponyms[:500]\n",
        "\n",
        "#Gets the cosine similarity of the two groups \n",
        "hyper_similar = cosine_gensim(lemmas2, hypernyms500)\n",
        "hypo_similar = cosine_gensim(lemmas2, hyponyms500)\n",
        "\n",
        "#Plot for cosine info\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot([syns_similar, ants_similar, hyper_similar, hypo_similar], widths=0.5)\n",
        "plt.xticks([1, 2, 3, 4], ['Synonyms', 'Antonyms', 'Hypernyms', 'Hyponyms'])\n",
        "plt.xlabel(\"Distribution\")\n",
        "plt.ylabel(\"Similarity\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Similarity')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o70I1GBbjSGL",
        "outputId": "25ef835c-1fd3-4783-afdc-8de4e0374221"
      },
      "source": [
        "#Some calculations for the data and analysis \n",
        "\n",
        "print(\"Data Analysis for Exercise 3:\")\n",
        "print(f'\\n')\n",
        "\n",
        "#Calculates means of data for each category\n",
        "synsim = round(np.mean(cosine_gensim(lemmas, synonyms500))*100)\n",
        "antsim = round(np.mean(cosine_gensim(lemmas, antonyms500))*100)\n",
        "hypersim = round(np.mean(cosine_gensim(lemmas2, hypernyms500))*100)\n",
        "hyposim = round(np.mean(cosine_gensim(lemmas2, hyponyms500))*100)\n",
        "\n",
        "print(f'Synonyms show a {synsim}% similarity to their lemmas')\n",
        "print(f'Antonyms show a {antsim}% similarity to their lemmas')\n",
        "print(f'Hypernyms show a {hypersim}% similarity to their lemmas')\n",
        "print(f'Hyponyms show a {hyposim}% similarity to their lemmas')\n",
        "print(f'There is a {synsim-antsim}% difference between the similarities of synonyms and antonyms')\n",
        "print(f'There is a {hypersim-hyposim}% difference between the similarities of hypernyms and hyponyms')\n",
        "print(f'\\n')\n",
        "\n",
        "#Data analysis for hypothesis explanations for part 3\n",
        "print(\"Data Analysis for Hypotheses Explanations in Exercise 3:\")\n",
        "print(f'\\n')\n",
        "\n",
        "#Calcuation for 'good' in hypothesis 1 explanation\n",
        "lemmas=[]\n",
        "synonyms=[]\n",
        "antonyms=[]\n",
        "\n",
        "word=\"good\"\n",
        "for syn in wordnet.synsets(word):\n",
        "  for l in syn.lemmas():\n",
        "    if len(wordnet.synsets(word)) > 0 and len(l.antonyms()) > 0:\n",
        "      lemmas.append(syn.lemma_names()[0])\n",
        "      synonyms.append(wordnet.synsets(word)[0].lemma_names()[0])\n",
        "      if l.antonyms():\n",
        "        antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "syns_good = round(np.mean(cosine_gensim(lemmas, synonyms))*100)\n",
        "ants_good = round(np.mean(cosine_gensim(lemmas, antonyms))*100)\n",
        "\n",
        "print(\"Calcuation for 'good' in hypothesis 1 explanation:\")\n",
        "print(f'The word good is {syns_good}% similar to its synonyms')\n",
        "print(f'The word good is {ants_good}% similar to its antonyms')\n",
        "print(f'\\n')\n",
        "\n",
        "#Calcuation for 'food/fruit/apple' in hypothesis 2 explanation\n",
        "food_fruit = round(np.mean(word2vec_gensim.similarity('food', 'fruit'))*100)\n",
        "food_apple = round(np.mean(word2vec_gensim.similarity('food', 'apple'))*100)\n",
        "print(\"Calcuation for 'food' in hypothesis 2 explanation:\")\n",
        "print(f'The word food is {food_fruit}% similar to fruit')\n",
        "print(f'The word food is {food_apple}% similar to apple')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Analysis for Exercise 3:\n",
            "\n",
            "\n",
            "Synonyms show a 55% similarity to their lemmas\n",
            "Antonyms show a 38% similarity to their lemmas\n",
            "Hypernyms show a 26% similarity to their lemmas\n",
            "Hyponyms show a 19% similarity to their lemmas\n",
            "There is a 17% difference between the similarities of synonyms and antonyms\n",
            "There is a 7% difference between the similarities of hypernyms and hyponyms\n",
            "\n",
            "\n",
            "Data Analysis for Hypotheses Explanations in Exercise 3:\n",
            "\n",
            "\n",
            "Calcuation for 'good' in hypothesis 1 explanation:\n",
            "The word good is 93% similar to its synonyms\n",
            "The word good is 36% similar to its antonyms\n",
            "\n",
            "\n",
            "Calcuation for 'food' in hypothesis 2 explanation:\n",
            "The word food is 37% similar to fruit\n",
            "The word food is 22% similar to apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6AKX3LXJ44L"
      },
      "source": [
        "# Exercise 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXNC6kj9oS4s"
      },
      "source": [
        "#Import city state list from Google Analogies \n",
        "master = \"https://raw.githubusercontent.com/MattyXarope/Homework-Comp-Semantics/main/citystate.txt\"\n",
        "req = requests.get(master)\n",
        "req = req.text\n",
        "\n",
        "#Split list into items\n",
        "citystate = req.split()\n",
        "\n",
        "#Separate items into lists \n",
        "city1 = citystate[0::4]\n",
        "state1 = citystate[1::4]\n",
        "city2 = citystate[2::4]\n",
        "state2 = citystate[3::4]\n",
        "\n",
        "#Define function to get the analogy using the first city state pair and the second city to get the second state \n",
        "def analogy(list1, list2, list3):\n",
        "  all_values = set(word2vec_gensim.vocab)\n",
        "  return(\n",
        "      [\n",
        "          word2vec_gensim.most_similar(positive=[three, two], negative=[one], topn=1)\n",
        "          if {one, two, three} <= all_values else 'None'\n",
        "          for one, two, three in zip(list1, list2, list3)\n",
        "      ]\n",
        "  )\n",
        "\n",
        "#Run the analogy function over the first city/state pair and the second city to get the second state\n",
        "x = analogy(city1,state1,city2)\n",
        "dict = {'City 1': city1, 'State 1': state1, 'City 2': city2, 'State 2': state2, 'State Word2Vec': x}\n",
        "\n",
        "#Save to a csv file, commented out because it's not necessary for others to have    \n",
        "#df = pd.DataFrame(dict) \n",
        "#df.to_csv('citystatevals.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lGbl7vBvUjs",
        "outputId": "a2810ce8-2957-473b-aec0-2cf766f579cf"
      },
      "source": [
        "#Analysis for Exercise 5\n",
        "\n",
        "print(\"Data Analysis for Exercise 5:\")\n",
        "print(f'\\n')\n",
        "\n",
        "#Import the cleaned CSV for city state comparison\n",
        "CSV_CITYSTATE = \"https://raw.githubusercontent.com/MattyXarope/Homework-Comp-Semantics/main/citystate.csv\"\n",
        "citystate_csv = pd.read_csv(CSV_CITYSTATE)\n",
        "\n",
        "#Set variables for comparisons of States 2 and States 2 W2V using CSV\n",
        "state2_w2v = citystate_csv['State 2  W2V']\n",
        "state2_plain = citystate_csv['State 2']\n",
        "\n",
        "#Info about \"None\" \n",
        "occurrences = Counter(state2_w2v)\n",
        "none_occurences = occurrences.get(\"None\")\n",
        "avg_none = round((none_occurences / (len(state2_w2v)) * 100))\n",
        "print(f'There were no matches for {avg_none}% ({none_occurences} instances) of {len(state2_w2v)} utterances.')\n",
        "\n",
        "\n",
        "#Accuracy without \"None\" included\n",
        "\n",
        "#Import the cleaned CSV for city state comparison with no None instances\n",
        "CSV_CITYSTATE_NO_NONE = \"https://raw.githubusercontent.com/MattyXarope/Homework-Comp-Semantics/main/citystate_no_none.csv\"\n",
        "citystate_csv_no_none = pd.read_csv(CSV_CITYSTATE_NO_NONE)\n",
        "\n",
        "#Set variables for comparisons of States 2 and States 2 W2V using CSV with no None\n",
        "state2_w2v_no_none = citystate_csv_no_none['State 2  W2V']\n",
        "state2_plain_no_none = citystate_csv_no_none['State 2']\n",
        "\n",
        "#Get accuracy of no None dataset\n",
        "accuracy = sum(1 for x,y in zip(state2_w2v_no_none,state2_plain_no_none) if x == y) / len(state2_w2v_no_none)\n",
        "accuracy2 = round(accuracy*100) \n",
        "print(f'The predictions from the Word2Vec model are {accuracy2}% accurate based on {len(state2_w2v_no_none)} utterances.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Analysis for Exercise 5:\n",
            "\n",
            "\n",
            "There were no matches for 36% (900 instances) of 2467 utterances.\n",
            "The predictions from the Word2Vec model are 78% accurate based on 1567 utterances.\n"
          ]
        }
      ]
    }
  ]
}