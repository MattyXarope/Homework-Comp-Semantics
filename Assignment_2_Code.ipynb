{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 - Code.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattyXarope/Homework-Comp-Semantics/blob/main/Assignment_2_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEnSJ30oLXat"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGPxhD8PCrHm",
        "outputId": "db3b96e0-65a1-4564-d2c6-8ebde82a9f5f"
      },
      "source": [
        "# -*- coding: utf-8 -*- \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('word2vec_sample')\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet \n",
        "import pandas as pd\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "import re\n",
        "import csv \n",
        "from itertools import chain\n",
        "import gensim\n",
        "\n",
        "path_to_word2vec_sample = nltk.data.find('models/word2vec_sample/pruned.word2vec.txt')\n",
        "word2vec_gensim = gensim.models.KeyedVectors.load_word2vec_format(path_to_word2vec_sample)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1kiZfEVK8Th"
      },
      "source": [
        "# Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grcHEvqOC0Mq",
        "outputId": "ca679914-046a-44d9-936d-ef3a04b3f2e1"
      },
      "source": [
        "## Exercise 1 ##\n",
        "\n",
        "#Tests to see how many senses there are for the word language \n",
        "word = 'language'\n",
        "synsets = wordnet.synsets(word, lang=\"eng\")\n",
        "n_synsets = len(synsets)\n",
        "\n",
        "#Prints information about synsets for the word \n",
        "print(f'Lemma English: {word}')\n",
        "print(f'Number of synsets: {n_synsets}')\n",
        "print(f'Synsets details: ')\n",
        "for ss in synsets:\n",
        "    print(f'Name: {ss.name()}')\n",
        "    print(f'Gloss: {ss.definition()}')\n",
        "    print(f'Lemmas English: {ss.lemmas()}')\n",
        "    print(f'\\n')\n",
        "    \n",
        "#Establishes a concordance list for the word language in the Brown corpus (commented out due to use of sketch engine)\n",
        "# corpus = brown.words()\n",
        "# text = Text(corpus)\n",
        "# con_list = text.concordance_list(\"language\", width=250, lines=100)\n",
        "# df = pd.DataFrame(data={\"Utterance\": con_list})\n",
        "# df.to_csv(\"./Brown100.csv\", sep=',',index=False)\n",
        "\n",
        "#Adds file from SketchEngine of 100 instances of 'language' in the Brown corpus \n",
        "\n",
        "CSV_BROWN = \"https://raw.githubusercontent.com/MattyXarope/Homework-Comp-Semantics/main/Brown100Final.csv\"\n",
        "lang_syn = pd.read_csv(CSV_BROWN)\n",
        "\n",
        "#Create contigency table using data from lang_syn\n",
        "adelle = lang_syn['adelle']\n",
        "matt = lang_syn['matt']\n",
        "annotations = pd.DataFrame({'Adelle': adelle, 'Matt': matt})\n",
        "\n",
        "#Print contigency table data\n",
        "print(f'******************\\n')\n",
        "print('contingency table: ')\n",
        "print(f'******************\\n\\n')\n",
        "# note the 'margins=True' parameter -- make sure you understand what it does\n",
        "print(pd.crosstab(annotations['Adelle'], annotations['Matt'], margins=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma English: language\n",
            "Number of synsets: 6\n",
            "Synsets details: \n",
            "Name: language.n.01\n",
            "Gloss: a systematic means of communicating by the use of sounds or conventional symbols\n",
            "Lemmas English: [Lemma('language.n.01.language'), Lemma('language.n.01.linguistic_communication')]\n",
            "\n",
            "\n",
            "Name: speech.n.02\n",
            "Gloss: (language) communication by word of mouth\n",
            "Lemmas English: [Lemma('speech.n.02.speech'), Lemma('speech.n.02.speech_communication'), Lemma('speech.n.02.spoken_communication'), Lemma('speech.n.02.spoken_language'), Lemma('speech.n.02.language'), Lemma('speech.n.02.voice_communication'), Lemma('speech.n.02.oral_communication')]\n",
            "\n",
            "\n",
            "Name: lyric.n.01\n",
            "Gloss: the text of a popular song or musical-comedy number\n",
            "Lemmas English: [Lemma('lyric.n.01.lyric'), Lemma('lyric.n.01.words'), Lemma('lyric.n.01.language')]\n",
            "\n",
            "\n",
            "Name: linguistic_process.n.02\n",
            "Gloss: the cognitive processes involved in producing and understanding linguistic communication\n",
            "Lemmas English: [Lemma('linguistic_process.n.02.linguistic_process'), Lemma('linguistic_process.n.02.language')]\n",
            "\n",
            "\n",
            "Name: language.n.05\n",
            "Gloss: the mental faculty or power of vocal communication\n",
            "Lemmas English: [Lemma('language.n.05.language'), Lemma('language.n.05.speech')]\n",
            "\n",
            "\n",
            "Name: terminology.n.01\n",
            "Gloss: a system of words used to name things in a particular discipline\n",
            "Lemmas English: [Lemma('terminology.n.01.terminology'), Lemma('terminology.n.01.nomenclature'), Lemma('terminology.n.01.language')]\n",
            "\n",
            "\n",
            "******************\n",
            "\n",
            "contingency table: \n",
            "******************\n",
            "\n",
            "\n",
            "Matt    s1  s2  s3  s4  s5  s6  All\n",
            "Adelle                             \n",
            "s1      53  10   0   3   0   4   70\n",
            "s2       7   1   0   0   0   0    8\n",
            "s3       0   0   1   0   0   0    1\n",
            "s4       4   0   0   0   0   0    4\n",
            "s5       2   0   0   0   1   1    4\n",
            "s6       0   5   0   0   0   8   13\n",
            "All     66  16   1   3   1  13  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bvpxa_PLLNM"
      },
      "source": [
        "# Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhuJT2dCDJFp",
        "outputId": "6c6a0ec9-879f-4d60-a2d8-5225d2c7b270"
      },
      "source": [
        "## Exercise 2 ## \n",
        "\n",
        "#S1 Info\n",
        "#Gloss: a systematic means of communicating by the use of sounds or conventional symbols\n",
        "#Percent of all utterances \n",
        "all_percentS1 = (53/100)*100\n",
        "print(f'S1 was agreed upon by both annotators in {all_percentS1}% of all utterances')\n",
        "\n",
        "#S2 Info\n",
        "#Gloss: (language) communication by word of mouth\n",
        "#Percent of all utterances \n",
        "all_percentS2 = (1/100)*100\n",
        "print(f'S2 was agreed upon by both annotators in {all_percentS2}% of all utterances')\n",
        "\n",
        "#S3 Info\n",
        "#Gloss: the text of a popular song or musical-comedy number\n",
        "#Percent of all utterances \n",
        "all_percentS3 = (1/100)*100\n",
        "print(f'S3 was agreed upon by both annotators in {all_percentS3}% of all utterances')\n",
        "\n",
        "#S4 Info\n",
        "#Gloss: the cognitive processes involved in producing and understanding linguistic communication\n",
        "#Percent of all utterances \n",
        "all_percentS4 = (0/100)*100\n",
        "print(f'S4 was agreed upon by both annotators in {all_percentS4}% of all utterances')\n",
        "\n",
        "#S5 Info\n",
        "#Gloss: the mental faculty or power of vocal communication\n",
        "#Percent of all utterances \n",
        "all_percentS5 = (1/100)*100\n",
        "print(f'S5 was agreed upon by both annotators in {all_percentS5}% of all utterances')\n",
        "\n",
        "#S6 Info\n",
        "#Gloss: a system of words used to name things in a particular discipline\n",
        "#Percent of all utterances \n",
        "all_percentS6 = (8/100)*100\n",
        "print(f'S6 was agreed upon by both annotators in {all_percentS6}% of all utterances')\n",
        "\n",
        "#Print General Info for this section\n",
        "all_agreements = all_percentS1 + all_percentS2 + all_percentS3 + all_percentS4 + all_percentS5 + all_percentS6\n",
        "print(f'The annotators agreed on {all_agreements}% of all utterances')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S1 was agreed upon by both annotators in 53.0% of all utterances\n",
            "S2 was agreed upon by both annotators in 1.0% of all utterances\n",
            "S3 was agreed upon by both annotators in 1.0% of all utterances\n",
            "S4 was agreed upon by both annotators in 0.0% of all utterances\n",
            "S5 was agreed upon by both annotators in 1.0% of all utterances\n",
            "S6 was agreed upon by both annotators in 8.0% of all utterances\n",
            "The annotators agreed on 64.0% of all utterances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc89QxxkLQzV"
      },
      "source": [
        "# Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "BTwY0cc9DaY5",
        "outputId": "9c1d14c4-0d29-4182-d7b2-b304a2871c1e"
      },
      "source": [
        "## Exercise 3 ## \n",
        "lemmas=[]\n",
        "synonyms=[]\n",
        "antonyms=[]\n",
        "\n",
        "#For loop to add synonyms from wordnet to list \n",
        "for word in wordnet.words(): #for every word in wordnet\n",
        "#    print (word,end=\": \") #print the word so we know what word it is \n",
        "    for syn in wordnet.synsets(word): #for every synset of a word 'word'\n",
        "      for l in syn.lemmas(): #and for every lemma for the word\n",
        "        if len(wordnet.synsets(word)) > 0 and len(l.antonyms()) > 0: #if the number of synsets(synonyms) is at least 1 and if the # of antonyms of the lemma is at least 1\n",
        "          lemmas.append(l.name()) #and add the lemma name to the list of lemmas\n",
        "          synonyms.append(wordnet.synsets(word)[0]) #add the first synset of the word to the list of synonyms         \n",
        "          antonyms.append(l.antonyms())#add the antonym of the lemma to the antonyms list                   \n",
        "#    print(synonyms, antonyms) #display the syns and ants to see what's going on  \n",
        "    if len(synonyms) > 500: #if there are more than 500 synonyms \n",
        "        break #then quit the process  \n",
        "\n",
        "#Fucntion to compare the cosine similarites in word2vec \n",
        "def cosine_gensim(list1, list2):\n",
        "    return word2vec_gensim.similarity(list1, list2)  \n",
        "\n",
        "#Checking the similarites with our lists\n",
        "\n",
        "#Words and synonyms\n",
        "print(cosine_gensim(lemmas, synonyms))\n",
        "\n",
        "#Words and antonyms\n",
        "print(cosine_gensim(lemmas, antonyms))\n",
        "\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-3b2cecc27674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#Words and synonyms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynonyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#Words and antonyms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-3b2cecc27674>\u001b[0m in \u001b[0;36mcosine_gensim\u001b[0;34m(list1, list2)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#Fucntion to compare the cosine similarites in word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mword2vec_gensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#Checking the similarites with our lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \"\"\"\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'a_la_carte' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7RCPqurkd8p",
        "outputId": "70b613dd-7459-4fd1-c710-cc7df2a42c4a"
      },
      "source": [
        "print(len(lemmas))\n",
        "print(len(synonyms))\n",
        "print(len(antonyms))\n",
        "\n",
        "print(lemmas)\n",
        "print(synonyms)\n",
        "print(antonyms)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n",
            "508\n",
            "508\n",
            "['a_la_carte', 'a_posteriori', 'a_posteriori', 'a_priori', 'a_priori', 'abactinal', 'abaxial', 'contract', 'abducent', 'abduct', 'abducent', 'abient', 'able', 'abnormal', 'abnormal', 'aboral', 'native', 'above', 'above', 'honestly', 'abranchiate', 'abranchiate', 'abranchiate', 'contract', 'abridged', 'absent', 'absolute', 'absolve', 'absorbable', 'absorb', 'absorbent', 'absorb', 'absorbent', 'abstemious', 'nondrinker', 'abstract', 'abundant', 'abused', 'acarpelous', 'acarpelous', 'acatalectic', 'acaudate', 'acaudate', 'acaulescent', 'accelerate', 'accelerate', 'tonic', 'stressed', 'accentual', 'acceptable', 'accept', 'accept', 'accept', 'accept', 'accept', 'accept', 'acceptive', 'accessible', 'oblige', 'accommodating', 'accommodating', 'accompanied', 'accompanied', 'accordant', 'accurate', 'accusatorial', 'accustomed', 'noncellular', 'achlamydeous', 'achondritic', 'achromatic', 'achromatinic', 'acid-loving', 'acidic', 'admit', 'acknowledged', 'win', 'acquisitive', 'acquit', 'acrocarpous', 'acropetal', 'acroscopic', 'actable', 'actinal', 'act', 'actinomorphic', 'actinomorphic', 'activate', 'activate', 'active_voice', 'active', 'active', 'active', 'active', 'active', 'active', 'active', 'active', 'active', 'active', 'active', 'actual', 'acute', 'acute', 'acyclic', 'acyclic', 'adaptable', 'adaptive', 'adaptive', 'adaxial', 'addicted', 'addictive', 'linear', 'additive', 'addressed', 'adducent', 'adduct', 'adducent', 'adducent', 'adequate', 'adhesive', 'adiabatic', 'adient', 'adjective', 'align', 'adjusted', 'adjusted', 'admire', 'admissible', 'adnate', 'adoptable', 'adopted', 'adoptive', 'adopted', 'adorned', 'adroit', 'adscript', 'adsorbable', 'adsorbable', 'adsorbent', 'adsorbent', 'adult', 'adulterating', 'adulterating', 'advance', 'advance', 'advance', 'gain', 'progress', 'promote', 'advance', 'advance', 'gain', 'progress', 'promote', 'advance', 'advance', 'gain', 'progress', 'promote', 'advantageous', 'adventurous', 'adventurous', 'advisable', 'well-advised', 'undynamic', 'aerobic', 'aerobic', 'aerobic', 'aerobic', 'aesthetic', 'aesthetic', 'afebrile', 'affected', 'affected', 'moved', 'afferent', 'affirmative', 'affirmative', 'affirmative', 'affixed', 'tributary', 'afloat', 'afraid', 'aft', 'aft', 'aged', 'age', 'age', 'worsen', 'worsen', 'aggressive', 'age', 'agitate', 'agitated', 'agitated', 'agitate', 'agnostic', 'agnostic', 'agreeable', 'agree', 'agree', 'aground', 'ahead', 'forward', 'ahistorical', 'assisted', 'air-to-air', 'air-to-surface', 'air-to-surface', 'airworthy', 'alarming', 'alcoholic', 'alert', 'outside', 'outdoors', 'algorithmic', 'noncitizen', 'stranger', 'alienable', 'align', 'aligned', 'align', 'alike', 'aliquot', 'alive', 'animated', 'alkaline', 'alkaline', 'alkaline-loving', 'all', 'wholly', 'allochthonous', 'allogeneic', 'allopathic', 'allopatric', 'permissible', 'alphabetic', 'alphabetic', 'alphabetic', 'alphabetic', 'alterable', 'altered', 'alternate', 'alternating', 'altricial', 'altruistic', 'amateur', 'ambidextrous', 'equivocal', 'ambiguous', 'unambitious', 'ambitious', 'ambiversive', 'better', 'better', 'better', 'amended', 'ametabolic', 'ametabolic', 'ametropic', 'amicable', 'imperfectly', 'amphibious', 'amphoteric', 'amphitropous', 'amphoteric', 'ample', 'anabatic', 'anabolic', 'anaclinal', 'anadromous', 'anaerobic', 'anaerobic', 'anaerobic', 'anal', 'anal', 'analogue', 'analogous', 'analogue', 'analphabetic', 'analphabetic', 'analytic', 'analytic', 'analytic', 'analytic', 'analytic', 'analyze', 'analyzed', 'anaphrodisiac', 'anastigmatic', 'anatropous', 'androgynous', 'anemophilous', 'anestrous', 'Anglican', 'angry', 'angular', 'angular', 'anhydrous', 'enliven', 'animate', 'animate', 'sentient', 'enliven', 'animated', 'enliven', 'anionic', 'anisotropic', 'annual', 'anodic', 'anodic', 'anonymous', 'anonymous', 'triclinic', 'antagonistic', 'antagonistic', 'ancestor', 'antecedent', 'antemeridian', 'antemortem', 'prenatal', 'prenuptial', 'prenatal', 'anterior', 'anterograde', 'anti', 'anti-American', 'anticholinergic', 'anticlimactic', 'anticlinal', 'counterclockwise', 'counterclockwise', 'antimagnetic', 'antipyretic', 'antiseptic', 'antonymous', 'antrorse', 'aperiodic', 'apetalous', 'aphrodisiac', 'aphrodisiac', 'aplacental', 'apocarpous', 'apocrine', 'apologetic', 'dress', 'appealable', 'attract', 'appealing', 'sympathetic', 'appendaged', 'appetizing', 'appetizing', 'lend_oneself', 'enforce', 'applied', 'appointive', 'appointive', 'opposable', 'appreciate', 'appreciate', 'approachable', 'appropriate', 'approve', 'approve', 'approval', 'approve', 'approve', 'apropos', 'apteral', 'aquatic', 'aqueous', 'arbitrable', 'arbitrary', 'arboreal', 'arboreal', 'arenaceous', 'argillaceous', 'argumentative', 'arm', 'armed', 'armed', 'armed', 'armless', 'armored', 'armored', 'armored', 'armored', 'wake_up', 'stimulate', 'energize', 'energise', 'awaken', 'arrange', 'arranged', 'artesian', 'disingenuous', 'artful', 'articulate', 'articulated', 'articulated', 'artificial', 'even-toed_ungulate', 'ingenuous', 'artless', 'ancestor', 'ancestor', 'ascend', 'rise', 'ascending', 'asexual', 'ashamed', 'asleep', 'declarative', 'declaratory', 'assertive', 'assigned', 'assimilate', 'assimilate', 'assimilate', 'assisted', 'associate', 'associative', 'associative', 'reassure', 'reassure', 'astigmatic', 'astomatous', 'astringent', 'asymmetrical', 'asymmetrical', 'asynchronous', 'asynchronous', 'asyndetic', 'nuclear', 'atomistic', 'atomistic', 'atonal', 'atonic', 'nontoxic', 'atrophied', 'attachable', 'attach', 'attach', 'attached', 'attached', 'attack', 'attend', 'accompanied', 'attentive', 'heedful', 'overdress', 'dress_up', 'attractive', 'attractive', 'attributable', 'attributive', 'atypical', 'audible', 'auspicious', 'authorized', 'authorized', 'autochthonous', 'autoecious', 'autogamous', 'autogamous', 'autogenous', 'autogenous', 'autologous', 'automatic', 'autotrophic', 'autotrophic', 'autumnal', 'available', 'avascular', 'avenged', 'evitable', 'evitable', 'avirulent', 'evitable', 'avow', 'wake_up', 'awake', 'awaken', 'wake_up', 'awakened', 'aware', 'mindful', 'away', 'awed', 'awed', 'awed', 'nasty', 'awkward', 'awned', 'awnless', 'awned', 'azonal', 'rear', 'back', 'back', 'back', 'back', 'back', 'backward', 'back', 'backward']\n",
            "[Synset('a_la_carte.n.01'), Synset('a_posteriori.a.01'), Synset('a_posteriori.a.01'), Synset('a_priori.a.01'), Synset('a_priori.a.01'), Synset('abactinal.a.01'), Synset('abaxial.a.01'), Synset('abridge.v.01'), Synset('abducent.n.01'), Synset('kidnap.v.01'), Synset('kidnap.v.01'), Synset('abient.a.01'), Synset('able.a.01'), Synset('abnormal.a.01'), Synset('abnormal.a.01'), Synset('aboral.a.01'), Synset('aborigine.n.02'), Synset('above.n.01'), Synset('above.n.01'), Synset('aboveboard.s.01'), Synset('abranchiate.a.01'), Synset('abranchiate.a.01'), Synset('abranchiate.a.01'), Synset('abridge.v.01'), Synset('abridge.v.01'), Synset('absent.v.01'), Synset('absolute.n.01'), Synset('shrive.v.01'), Synset('absorbable.a.01'), Synset('absorb.v.01'), Synset('absorbent_material.n.01'), Synset('absorb.v.01'), Synset('absorbent.a.01'), Synset('abstemious.a.01'), Synset('abstainer.n.02'), Synset('abstraction.n.01'), Synset('abundant.a.01'), Synset('mistreat.v.01'), Synset('acarpelous.a.01'), Synset('acarpelous.a.01'), Synset('acatalectic.n.01'), Synset('acaudate.a.01'), Synset('acaudate.a.01'), Synset('acaulescent.a.01'), Synset('accelerate.v.01'), Synset('accelerate.v.01'), Synset('stress.v.01'), Synset('stress.v.01'), Synset('accentual.a.01'), Synset('acceptable.a.01'), Synset('accept.v.01'), Synset('accept.v.01'), Synset('accept.v.01'), Synset('accept.v.01'), Synset('accept.v.01'), Synset('accept.v.01'), Synset('acceptive.a.01'), Synset('accessible.a.01'), Synset('suit.v.01'), Synset('suit.v.01'), Synset('accommodating.a.01'), Synset('attach_to.v.01'), Synset('attach_to.v.01'), Synset('accordant.a.01'), Synset('accurate.a.01'), Synset('accusatorial.a.01'), Synset('habituate.v.02'), Synset('noncellular.a.01'), Synset('achlamydeous.a.01'), Synset('achondritic.a.01'), Synset('achromatic.a.01'), Synset('achromatinic.a.01'), Synset('acid-loving.a.01'), Synset('acidic.a.01'), Synset('admit.v.01'), Synset('admit.v.01'), Synset('get.v.01'), Synset('acquisitive.a.01'), Synset('acquit.v.01'), Synset('acrocarpous.a.01'), Synset('acropetal.a.01'), Synset('acroscopic.a.01'), Synset('actable.a.01'), Synset('actinal.a.01'), Synset('acting.n.01'), Synset('actinomorphic.a.01'), Synset('actinomorphic.a.01'), Synset('trip.v.04'), Synset('energizing.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('active_agent.n.01'), Synset('actual.a.01'), Synset('acute_accent.n.01'), Synset('acute_accent.n.01'), Synset('acyclic.a.01'), Synset('acyclic.a.01'), Synset('adaptable.a.01'), Synset('adaptive.a.01'), Synset('adaptive.a.01'), Synset('adaxial.a.01'), Synset('addict.v.01'), Synset('addictive.a.01'), Synset('additive.n.01'), Synset('additive.n.01'), Synset('address.v.01'), Synset('adducent.a.01'), Synset('adduct.v.01'), Synset('adduct.v.01'), Synset('adducent.a.01'), Synset('adequate.a.01'), Synset('adhesive_material.n.01'), Synset('adiabatic.a.01'), Synset('adient.a.01'), Synset('adjective.n.01'), Synset('adjust.v.01'), Synset('adjust.v.01'), Synset('adjust.v.01'), Synset('admire.v.01'), Synset('admissible.a.01'), Synset('adnate.a.01'), Synset('adoptable.a.01'), Synset('adopt.v.01'), Synset('adoptive.a.01'), Synset('adoptive.a.01'), Synset('decorate.v.01'), Synset('adroit.a.01'), Synset('adscript.a.01'), Synset('adsorbable.a.01'), Synset('adsorbate.n.01'), Synset('adsorbent.n.01'), Synset('adsorbent.a.01'), Synset('adult.n.01'), Synset('adulterant.n.01'), Synset('load.v.05'), Synset('progress.n.03'), Synset('progress.n.03'), Synset('progress.n.03'), Synset('progress.n.03'), Synset('progress.n.03'), Synset('progress.n.03'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advance.v.01'), Synset('advantageous.a.01'), Synset('adventurous.a.01'), Synset('adventurous.a.01'), Synset('advisable.a.01'), Synset('rede.v.02'), Synset('undynamic.a.01'), Synset('aerobic.a.01'), Synset('aerobic.a.01'), Synset('aerobic.a.01'), Synset('aerobic.a.01'), Synset('aesthetic.n.01'), Synset('aesthetic.a.02'), Synset('afebrile.a.01'), Synset('affect.v.01'), Synset('affect.v.01'), Synset('affect.v.01'), Synset('sensory_nerve.n.01'), Synset('affirmative.n.01'), Synset('affirmative.n.01'), Synset('affirmative.a.01'), Synset('affix.v.01'), Synset('affluent.n.01'), Synset('adrift.s.01'), Synset('afraid.a.01'), Synset('aft.a.01'), Synset('aft.a.01'), Synset('aged.n.01'), Synset('aged.n.01'), Synset('ripening.n.02'), Synset('worsen.v.02'), Synset('worsen.v.02'), Synset('aggressive.a.01'), Synset('ripening.n.02'), Synset('agitate.v.01'), Synset('agitate.v.01'), Synset('agitate.v.01'), Synset('agitate.v.01'), Synset('agnostic.n.01'), Synset('agnostic.a.02'), Synset('agreeable.a.01'), Synset('agree.v.01'), Synset('agree.v.01'), Synset('aground.a.01'), Synset('ahead.s.01'), Synset('ahead.s.01'), Synset('ahistorical.a.01'), Synset('help.v.01'), Synset('air-to-air.a.01'), Synset('air-to-surface.a.01'), Synset('air-to-surface.a.01'), Synset('airworthy.a.01'), Synset('dismay.v.02'), Synset('alcoholic.n.01'), Synset('alert.n.01'), Synset('alfresco.s.01'), Synset('alfresco.s.01'), Synset('algorithmic.a.01'), Synset('foreigner.n.01'), Synset('foreigner.n.01'), Synset('alienable.a.01'), Synset('align.v.01'), Synset('align.v.01'), Synset('align.v.01'), Synset('alike.a.01'), Synset('aliquot.n.01'), Synset('alive.a.01'), Synset('alive.a.01'), Synset('alkaline.a.01'), Synset('alkaline.a.01'), Synset('alkaline-loving.a.01'), Synset('all.a.01'), Synset('all.a.01'), Synset('allochthonous.a.01'), Synset('allogeneic.a.01'), Synset('allopathic.a.01'), Synset('allopatric.a.01'), Synset('allowable.s.01'), Synset('alphabetic.a.01'), Synset('alphabetic.a.01'), Synset('alphabetic.a.01'), Synset('alphabetic.a.01'), Synset('alterable.a.01'), Synset('change.v.01'), Synset('surrogate.n.01'), Synset('alternate.v.01'), Synset('altricial.a.01'), Synset('altruistic.a.01'), Synset('amateur.n.01'), Synset('ambidextrous.a.01'), Synset('equivocal.a.01'), Synset('equivocal.a.01'), Synset('unambitious.a.01'), Synset('ambitious.a.01'), Synset('ambiversive.a.01'), Synset('better.v.02'), Synset('better.v.02'), Synset('amend.v.01'), Synset('amend.v.01'), Synset('ametabolic.a.01'), Synset('ametabolic.a.01'), Synset('ametropic.a.01'), Synset('amicable.a.01'), Synset('amiss.s.01'), Synset('amphibious.a.01'), Synset('amphoteric.a.01'), Synset('amphitropous.a.01'), Synset('amphoteric.a.01'), Synset('ample.a.01'), Synset('anabatic.a.01'), Synset('anabolic.a.01'), Synset('anaclinal.a.01'), Synset('anadromous.a.01'), Synset('anaerobic.a.01'), Synset('anaerobic.a.01'), Synset('anaerobic.a.01'), Synset('anal.a.01'), Synset('anal.a.02'), Synset('analogue.n.01'), Synset('analogous.s.01'), Synset('analogue.n.01'), Synset('analphabet.n.01'), Synset('analphabet.n.01'), Synset('analytic.a.01'), Synset('analytic.a.01'), Synset('analytic.a.01'), Synset('analytic.a.02'), Synset('analytic.a.02'), Synset('analyze.v.01'), Synset('analyze.v.01'), Synset('anaphrodisiac.a.01'), Synset('anastigmatic.a.01'), Synset('anatropous.a.01'), Synset('androgynous.a.01'), Synset('anemophilous.a.01'), Synset('anestrous.a.01'), Synset('anglican.n.01'), Synset('angry.a.01'), Synset('angular.a.01'), Synset('angulate.v.01'), Synset('anhydrous.a.01'), Synset('inspire.v.01'), Synset('inspire.v.01'), Synset('inspire.v.01'), Synset('inspire.v.01'), Synset('inspire.v.01'), Synset('inspire.v.01'), Synset('inspire.v.01'), Synset('anionic_detergent.n.01'), Synset('anisotropic.a.01'), Synset('annual.n.01'), Synset('anodic.a.01'), Synset('anodic.a.01'), Synset('anonymous.a.01'), Synset('anonymous.a.01'), Synset('triclinic.a.01'), Synset('antagonistic.s.01'), Synset('antagonistic.s.01'), Synset('ancestor.n.01'), Synset('ancestor.n.01'), Synset('antemeridian.a.01'), Synset('antemortem.a.01'), Synset('prenatal.a.01'), Synset('prenuptial.a.01'), Synset('prenatal.a.01'), Synset('front_tooth.n.01'), Synset('anterograde.a.01'), Synset('anti.n.01'), Synset('anti-american.n.01'), Synset('anticholinergic.n.01'), Synset('anticlimactic.a.01'), Synset('anticlinal.a.01'), Synset('counterclockwise.a.01'), Synset('counterclockwise.a.01'), Synset('antimagnetic.a.01'), Synset('antipyretic.n.01'), Synset('antiseptic.n.01'), Synset('antonymous.a.01'), Synset('antrorse.a.01'), Synset('aperiodic.a.01'), Synset('apetalous.a.01'), Synset('aphrodisiac.n.01'), Synset('aphrodisiac.a.01'), Synset('aplacental.a.01'), Synset('apocarpous.a.01'), Synset('apocrine.a.01'), Synset('apologetic.a.01'), Synset('dress.v.02'), Synset('appealable.a.01'), Synset('appeal.v.01'), Synset('appeal.v.01'), Synset('appeal.v.01'), Synset('appendaged.a.01'), Synset('appetizing.a.01'), Synset('appetizing.a.01'), Synset('use.v.01'), Synset('use.v.01'), Synset('use.v.01'), Synset('appoint.v.01'), Synset('appointive.a.01'), Synset('opposable.a.01'), Synset('appreciate.v.01'), Synset('appreciate.v.01'), Synset('accessible.s.02'), Synset('allow.v.04'), Synset('approve.v.01'), Synset('approve.v.01'), Synset('blessing.n.01'), Synset('blessing.n.01'), Synset('blessing.n.01'), Synset('apropos.a.01'), Synset('apteral.a.01'), Synset('aquatic.n.01'), Synset('aqueous.a.01'), Synset('arbitrable.a.01'), Synset('arbitrary.a.01'), Synset('arborical.a.01'), Synset('arboreal.a.02'), Synset('arenaceous.a.01'), Synset('argillaceous.a.01'), Synset('argumentative.a.01'), Synset('arm.v.01'), Synset('arm.v.01'), Synset('arm.v.01'), Synset('arm.v.01'), Synset('armless.a.01'), Synset('armor.v.01'), Synset('armor.v.01'), Synset('armor.v.01'), Synset('armor.v.01'), Synset('arouse.v.01'), Synset('arouse.v.01'), Synset('arouse.v.01'), Synset('arouse.v.01'), Synset('arouse.v.01'), Synset('arrange.v.01'), Synset('arrange.v.01'), Synset('artesian.a.01'), Synset('disingenuous.a.01'), Synset('disingenuous.a.01'), Synset('joint.v.02'), Synset('joint.v.02'), Synset('joint.v.02'), Synset('artificial.a.01'), Synset('even-toed_ungulate.n.01'), Synset('ingenuous.a.01'), Synset('ingenuous.a.01'), Synset('ascendant.n.01'), Synset('ascendant.n.01'), Synset('rise.n.02'), Synset('rise.n.02'), Synset('rise.n.02'), Synset('asexual.a.01'), Synset('ashamed.a.01'), Synset('asleep.a.01'), Synset('assert.v.01'), Synset('assert.v.01'), Synset('assertive.a.01'), Synset('delegate.v.02'), Synset('absorb.v.02'), Synset('absorb.v.02'), Synset('absorb.v.02'), Synset('help.v.01'), Synset('associate.n.01'), Synset('associative.a.01'), Synset('associative.a.01'), Synset('guarantee.v.02'), Synset('guarantee.v.02'), Synset('astigmatic.a.01'), Synset('astomatous.a.01'), Synset('astringent.n.01'), Synset('asymmetrical.a.01'), Synset('asymmetrical.a.01'), Synset('asynchronous.a.01'), Synset('asynchronous.a.01'), Synset('asyndetic.a.01'), Synset('atomic.a.01'), Synset('atomistic.a.01'), Synset('atomistic.a.01'), Synset('atonal.a.01'), Synset('atonic.a.01'), Synset('nontoxic.a.01'), Synset('atrophy.v.01'), Synset('attachable.a.01'), Synset('attach.v.01'), Synset('attach.v.01'), Synset('attach.v.01'), Synset('attach.v.01'), Synset('attack.v.01'), Synset('attend.v.01'), Synset('attend.v.01'), Synset('attentive.a.01'), Synset('attentive.a.01'), Synset('overdress.v.02'), Synset('overdress.v.02'), Synset('attractive.a.01'), Synset('attractive.a.01'), Synset('attributable.a.01'), Synset('attributive.a.01'), Synset('atypical.a.01'), Synset('audible.n.01'), Synset('auspicious.a.01'), Synset('empower.v.01'), Synset('authorize.v.01'), Synset('autochthonous.a.01'), Synset('autoecious.a.01'), Synset('autogamous.a.01'), Synset('autogamous.a.01'), Synset('autogenous.a.01'), Synset('autogenous.a.01'), Synset('autologous.a.01'), Synset('automatic_rifle.n.01'), Synset('autotrophic.a.01'), Synset('autotrophic.a.01'), Synset('autumnal.a.01'), Synset('available.a.01'), Synset('avascular.a.01'), Synset('revenge.v.01'), Synset('evitable.a.01'), Synset('evitable.a.01'), Synset('avirulent.a.01'), Synset('evitable.a.01'), Synset('affirm.v.02'), Synset('wake_up.v.02'), Synset('wake_up.v.02'), Synset('awaken.v.01'), Synset('awaken.v.01'), Synset('awaken.v.01'), Synset('aware.a.01'), Synset('aware.a.01'), Synset('away.s.01'), Synset('awe.v.01'), Synset('awed.a.02'), Synset('awed.a.02'), Synset('atrocious.s.02'), Synset('awkward.s.01'), Synset('awned.a.01'), Synset('awnless.a.01'), Synset('awned.a.01'), Synset('azonal.a.01'), Synset('back.n.01'), Synset('back.n.01'), Synset('back.n.01'), Synset('back.n.01'), Synset('back.n.01'), Synset('back.n.01'), Synset('back.n.01'), Synset('back.n.01'), Synset('back.n.01')]\n",
            "[[Lemma('table_d'hote.a.01.table_d'hote')], [Lemma('a_priori.a.01.a_priori')], [Lemma('a_priori.r.01.a_priori')], [Lemma('a_posteriori.a.01.a_posteriori')], [Lemma('a_posteriori.r.01.a_posteriori')], [Lemma('actinal.a.01.actinal')], [Lemma('adaxial.a.01.adaxial')], [Lemma('elaborate.v.01.expand')], [Lemma('adducent.a.01.adducent')], [Lemma('adduct.v.01.adduct')], [Lemma('adducent.a.01.adducent')], [Lemma('adient.a.01.adient')], [Lemma('unable.a.01.unable')], [Lemma('normal.a.01.normal')], [Lemma('normal.a.03.normal')], [Lemma('oral.a.03.oral')], [Lemma('nonnative.a.01.nonnative')], [Lemma('below.r.02.below')], [Lemma('below.r.01.below')], [Lemma('dishonestly.r.01.dishonestly')], [Lemma('branchiate.a.01.branchiate')], [Lemma('branchiate.a.01.branchiate')], [Lemma('branchiate.a.01.branchiate')], [Lemma('elaborate.v.01.expand')], [Lemma('unabridged.a.01.unabridged')], [Lemma('present.a.02.present')], [Lemma('relative.a.01.relative')], [Lemma('blame.v.01.blame')], [Lemma('adsorbable.a.01.adsorbable')], [Lemma('emit.v.02.emit')], [Lemma('nonabsorbent.a.01.nonabsorbent')], [Lemma('emit.v.02.emit')], [Lemma('nonabsorbent.a.01.nonabsorbent')], [Lemma('gluttonous.a.01.gluttonous')], [Lemma('drinker.n.02.drinker')], [Lemma('concrete.a.01.concrete')], [Lemma('scarce.a.01.scarce')], [Lemma('unabused.a.01.unabused')], [Lemma('carpellate.a.01.carpellate')], [Lemma('carpellate.a.01.carpellate')], [Lemma('catalectic.a.01.catalectic'), Lemma('hypercatalectic.a.01.hypercatalectic')], [Lemma('caudate.a.01.caudate')], [Lemma('caudate.a.01.caudate')], [Lemma('caulescent.a.01.caulescent')], [Lemma('decelerate.v.01.decelerate')], [Lemma('decelerate.v.02.decelerate')], [Lemma('atonic.a.02.atonic')], [Lemma('unstressed.a.01.unstressed')], [Lemma('quantitative.a.03.quantitative'), Lemma('syllabic.a.03.syllabic')], [Lemma('unacceptable.a.02.unacceptable')], [Lemma('reject.v.01.reject')], [Lemma('refuse.v.02.refuse')], [Lemma('refuse.v.01.refuse')], [Lemma('reject.v.01.reject')], [Lemma('refuse.v.02.refuse')], [Lemma('refuse.v.01.refuse')], [Lemma('rejective.a.01.rejective')], [Lemma('inaccessible.a.01.inaccessible')], [Lemma('disoblige.v.02.disoblige')], [Lemma('unaccommodating.a.01.unaccommodating')], [Lemma('unaccommodating.a.01.unaccommodating')], [Lemma('unaccompanied.a.01.unaccompanied')], [Lemma('unaccompanied.a.02.unaccompanied')], [Lemma('discordant.a.01.discordant')], [Lemma('inaccurate.a.01.inaccurate')], [Lemma('inquisitorial.a.01.inquisitorial')], [Lemma('unaccustomed.a.01.unaccustomed')], [Lemma('cellular.a.02.cellular')], [Lemma('chlamydeous.a.01.chlamydeous')], [Lemma('chondritic.a.01.chondritic')], [Lemma('chromatic.a.03.chromatic')], [Lemma('chromatinic.a.01.chromatinic')], [Lemma('alkaline-loving.a.01.alkaline-loving')], [Lemma('alkaline.a.01.alkaline'), Lemma('amphoteric.a.01.amphoteric')], [Lemma('deny.v.01.deny')], [Lemma('unacknowledged.a.01.unacknowledged')], [Lemma('lose.v.08.lose')], [Lemma('unacquisitive.a.01.unacquisitive')], [Lemma('convict.v.01.convict')], [Lemma('pleurocarpous.a.01.pleurocarpous')], [Lemma('basipetal.a.01.basipetal')], [Lemma('basiscopic.a.01.basiscopic')], [Lemma('unactable.a.01.unactable')], [Lemma('abactinal.a.01.abactinal')], [Lemma('refrain.v.01.refrain')], [Lemma('zygomorphic.a.01.zygomorphic')], [Lemma('zygomorphic.a.01.zygomorphic')], [Lemma('inactivate.v.02.inactivate')], [Lemma('inactivate.v.02.inactivate')], [Lemma('passive_voice.n.01.passive_voice')], [Lemma('inactive.a.02.inactive')], [Lemma('passive.a.01.passive')], [Lemma('inactive.a.09.inactive')], [Lemma('inactive.a.04.inactive')], [Lemma('inactive.a.07.inactive')], [Lemma('quiet.a.06.quiet')], [Lemma('passive.a.03.passive')], [Lemma('stative.a.01.stative')], [Lemma('extinct.a.02.extinct')], [Lemma('dormant.a.02.dormant')], [Lemma('inactive.a.08.inactive')], [Lemma('potential.a.01.potential')], [Lemma('chronic.a.01.chronic')], [Lemma('obtuse.a.01.obtuse')], [Lemma('cyclic.a.02.cyclic')], [Lemma('cyclic.a.03.cyclic')], [Lemma('unadaptable.a.01.unadaptable')], [Lemma('maladaptive.a.01.maladaptive')], [Lemma('maladaptive.a.01.maladaptive')], [Lemma('abaxial.a.01.abaxial')], [Lemma('unaddicted.a.01.unaddicted')], [Lemma('nonaddictive.a.01.nonaddictive')], [Lemma('nonlinear.a.01.nonlinear')], [Lemma('subtractive.a.01.subtractive')], [Lemma('unaddressed.a.01.unaddressed')], [Lemma('abducent.a.01.abducent')], [Lemma('abduct.v.02.abduct')], [Lemma('abducent.a.01.abducent')], [Lemma('abducent.a.01.abducent')], [Lemma('inadequate.a.01.inadequate')], [Lemma('nonadhesive.a.01.nonadhesive')], [Lemma('diabatic.a.01.diabatic')], [Lemma('abient.a.01.abient')], [Lemma('substantive.a.02.substantive')], [Lemma('skew.v.01.skew')], [Lemma('unadjusted.a.01.unadjusted')], [Lemma('maladjusted.a.01.maladjusted')], [Lemma('look_down_on.v.01.look_down_on')], [Lemma('inadmissible.a.01.inadmissible')], [Lemma('connate.a.01.connate')], [Lemma('unadoptable.a.01.unadoptable')], [Lemma('native.a.02.native')], [Lemma('biological.a.02.biological')], [Lemma('native.a.02.native')], [Lemma('unadorned.a.01.unadorned')], [Lemma('maladroit.a.01.maladroit')], [Lemma('subscript.a.01.subscript'), Lemma('superscript.a.01.superscript')], [Lemma('absorbable.a.01.absorbable')], [Lemma('absorbable.a.01.absorbable')], [Lemma('nonadsorbent.a.01.nonadsorbent')], [Lemma('nonadsorbent.a.01.nonadsorbent')], [Lemma('juvenile.n.01.juvenile')], [Lemma('purifying.a.02.purifying')], [Lemma('purifying.a.02.purifying')], [Lemma('retreat.n.07.retreat')], [Lemma('withdraw.v.01.recede')], [Lemma('back.v.04.back')], [Lemma('fall_back.v.04.fall_back')], [Lemma('regress.v.03.regress')], [Lemma('demote.v.01.demote')], [Lemma('withdraw.v.01.recede')], [Lemma('back.v.04.back')], [Lemma('fall_back.v.04.fall_back')], [Lemma('regress.v.03.regress')], [Lemma('demote.v.01.demote')], [Lemma('withdraw.v.01.recede')], [Lemma('back.v.04.back')], [Lemma('fall_back.v.04.fall_back')], [Lemma('regress.v.03.regress')], [Lemma('demote.v.01.demote')], [Lemma('disadvantageous.a.01.disadvantageous')], [Lemma('unadventurous.a.01.unadventurous')], [Lemma('unadventurous.a.01.unadventurous')], [Lemma('inadvisable.a.01.inadvisable')], [Lemma('ill-advised.a.01.ill-advised')], [Lemma('dynamic.a.01.dynamic')], [Lemma('anaerobic.a.01.anaerobic')], [Lemma('anaerobic.a.02.anaerobic')], [Lemma('anaerobic.a.01.anaerobic')], [Lemma('anaerobic.a.01.anaerobic')], [Lemma('inaesthetic.a.01.inaesthetic')], [Lemma('inaesthetic.a.01.inaesthetic')], [Lemma('febrile.a.01.febrile')], [Lemma('unaffected.a.01.unaffected')], [Lemma('unaffected.a.04.unaffected')], [Lemma('unmoved.a.01.unmoved')], [Lemma('efferent.a.01.efferent')], [Lemma('negative.n.01.negative')], [Lemma('negative.a.02.negative')], [Lemma('negative.a.02.negative')], [Lemma('unaffixed.a.01.unaffixed')], [Lemma('distributary.n.01.distributary')], [Lemma('aground.a.01.aground')], [Lemma('unafraid.a.01.unafraid')], [Lemma('fore.a.01.fore')], [Lemma('fore.r.01.fore')], [Lemma('young.n.09.young')], [Lemma('rejuvenate.v.03.rejuvenate')], [Lemma('rejuvenate.v.03.rejuvenate')], [Lemma('better.v.02.better')], [Lemma('better.v.02.better')], [Lemma('unaggressive.a.01.unaggressive')], [Lemma('rejuvenate.v.03.rejuvenate')], [Lemma('calm.v.01.calm')], [Lemma('unagitated.a.02.unagitated')], [Lemma('unagitated.a.01.unagitated')], [Lemma('calm.v.01.calm')], [Lemma('gnostic.a.02.gnostic')], [Lemma('gnostic.a.02.gnostic')], [Lemma('disagreeable.a.01.disagreeable')], [Lemma('disagree.v.01.disagree')], [Lemma('disagree.v.02.disagree')], [Lemma('afloat.a.02.afloat')], [Lemma('back.r.04.back')], [Lemma('back.r.04.backward')], [Lemma('historical.a.01.historical')], [Lemma('unassisted.a.02.unassisted')], [Lemma('air-to-surface.a.01.air-to-surface'), Lemma('surface-to-air.a.01.surface-to-air')], [Lemma('air-to-air.a.01.air-to-air'), Lemma('surface-to-air.a.01.surface-to-air')], [Lemma('air-to-air.a.01.air-to-air'), Lemma('surface-to-air.a.01.surface-to-air')], [Lemma('unairworthy.a.01.unairworthy')], [Lemma('unalarming.a.01.unalarming')], [Lemma('nonalcoholic.a.01.nonalcoholic')], [Lemma('unalert.a.01.unalert')], [Lemma('inside.r.01.inside')], [Lemma('inside.r.01.indoors')], [Lemma('heuristic.a.01.heuristic')], [Lemma('citizen.n.01.citizen')], [Lemma('acquaintance.n.03.acquaintance')], [Lemma('inalienable.a.01.inalienable')], [Lemma('skew.v.01.skew')], [Lemma('nonaligned.a.01.nonaligned')], [Lemma('skew.v.01.skew')], [Lemma('unalike.a.01.unalike')], [Lemma('aliquant.n.01.aliquant')], [Lemma('dead.a.01.dead')], [Lemma('unanimated.a.01.unanimated')], [Lemma('acidic.a.01.acidic'), Lemma('amphoteric.a.01.amphoteric')], [Lemma('acidic.a.01.acidic'), Lemma('amphoteric.a.01.amphoteric')], [Lemma('acid-loving.a.01.acid-loving')], [Lemma('no.a.01.no'), Lemma('some.a.01.some')], [Lemma('partially.r.01.partly')], [Lemma('autochthonous.a.01.autochthonous')], [Lemma('xenogeneic.a.01.xenogeneic')], [Lemma('homeopathic.a.01.homeopathic')], [Lemma('sympatric.a.01.sympatric')], [Lemma('impermissible.a.01.impermissible')], [Lemma('analphabetic.a.01.analphabetic')], [Lemma('analphabetic.a.02.analphabetic')], [Lemma('analphabetic.a.01.analphabetic')], [Lemma('analphabetic.a.02.analphabetic')], [Lemma('unalterable.a.01.unalterable')], [Lemma('unaltered.a.01.unaltered')], [Lemma('opposite.a.02.opposite')], [Lemma('direct.a.07.direct')], [Lemma('precocial.a.01.precocial')], [Lemma('egoistic.a.01.egoistic')], [Lemma('professional.n.02.professional')], [Lemma('left-handed.a.01.left-handed'), Lemma('right-handed.a.01.right-handed')], [Lemma('unequivocal.a.01.unequivocal')], [Lemma('unambiguous.a.01.unambiguous')], [Lemma('ambitious.a.01.ambitious')], [Lemma('unambitious.a.01.unambitious')], [Lemma('extroversive.a.01.extroversive'), Lemma('introversive.a.01.introversive')], [Lemma('worsen.v.02.worsen')], [Lemma('worsen.v.01.worsen')], [Lemma('worsen.v.02.worsen')], [Lemma('unamended.a.01.unamended')], [Lemma('metabolic.a.02.metabolic')], [Lemma('metabolic.a.02.metabolic')], [Lemma('emmetropic.a.01.emmetropic')], [Lemma('hostile.a.01.hostile')], [Lemma('perfectly.r.02.perfectly')], [Lemma('aquatic.a.02.aquatic'), Lemma('terrestrial.a.03.terrestrial')], [Lemma('acidic.a.01.acidic'), Lemma('alkaline.a.01.alkaline')], [Lemma('anatropous.a.01.anatropous')], [Lemma('acidic.a.01.acidic'), Lemma('alkaline.a.01.alkaline')], [Lemma('meager.a.01.meager')], [Lemma('katabatic.a.01.katabatic')], [Lemma('catabolic.a.02.catabolic')], [Lemma('cataclinal.a.01.cataclinal')], [Lemma('catadromous.a.01.catadromous'), Lemma('diadromous.a.01.diadromous')], [Lemma('aerobic.a.01.aerobic')], [Lemma('aerobic.a.02.aerobic')], [Lemma('aerobic.a.01.aerobic')], [Lemma('oral.a.04.oral')], [Lemma('oral.a.04.oral')], [Lemma('digital.a.03.digital')], [Lemma('heterologous.a.01.heterologous'), Lemma('homologous.a.01.homologous')], [Lemma('digital.a.03.digital')], [Lemma('alphabetic.a.01.alphabetic')], [Lemma('alphabetic.a.02.alphabetic')], [Lemma('synthetic.a.02.synthetic')], [Lemma('synthetic.a.03.synthetic')], [Lemma('synthetic.a.04.synthetic')], [Lemma('synthetic.a.02.synthetic')], [Lemma('synthetic.a.04.synthetic')], [Lemma('synthesize.v.01.synthesize')], [Lemma('unanalyzed.a.01.unanalyzed')], [Lemma('aphrodisiac.a.01.aphrodisiac')], [Lemma('astigmatic.a.01.astigmatic')], [Lemma('amphitropous.a.01.amphitropous')], [Lemma('female.a.01.female'), Lemma('male.a.01.male')], [Lemma('entomophilous.a.01.entomophilous')], [Lemma('estrous.a.01.estrous')], [Lemma('nonconformist.n.01.Nonconformist')], [Lemma('unangry.a.01.unangry')], [Lemma('rounded.a.01.rounded')], [Lemma('rounded.a.01.rounded')], [Lemma('hydrous.a.01.hydrous')], [Lemma('deaden.v.06.deaden')], [Lemma('inanimate.a.01.inanimate')], [Lemma('inanimate.a.02.inanimate')], [Lemma('insentient.a.01.insentient')], [Lemma('deaden.v.06.deaden')], [Lemma('unanimated.a.01.unanimated')], [Lemma('deaden.v.06.deaden')], [Lemma('cationic.a.01.cationic')], [Lemma('isotropic.a.01.isotropic')], [Lemma('biennial.a.01.biennial'), Lemma('perennial.a.01.perennial')], [Lemma('cathodic.a.01.cathodic')], [Lemma('cathodic.a.01.cathodic')], [Lemma('onymous.a.01.onymous')], [Lemma('onymous.a.01.onymous')], [Lemma('monoclinic.a.01.monoclinic')], [Lemma('conciliatory.a.02.conciliatory')], [Lemma('synergistic.a.01.synergistic')], [Lemma('descendant.n.01.descendant')], [Lemma('subsequent.a.01.subsequent')], [Lemma('postmeridian.a.01.postmeridian')], [Lemma('postmortem.a.01.postmortem')], [Lemma('perinatal.a.01.perinatal'), Lemma('postnatal.a.01.postnatal')], [Lemma('postnuptial.a.01.postnuptial')], [Lemma('perinatal.a.01.perinatal'), Lemma('postnatal.a.01.postnatal')], [Lemma('posterior.a.01.posterior')], [Lemma('retrograde.a.02.retrograde')], [Lemma('pro.a.01.pro')], [Lemma('pro-american.a.01.pro-American')], [Lemma('cholinergic.a.01.cholinergic')], [Lemma('climactic.a.01.climactic')], [Lemma('synclinal.a.01.synclinal')], [Lemma('clockwise.a.01.clockwise')], [Lemma('clockwise.r.01.clockwise')], [Lemma('magnetic.a.02.magnetic')], [Lemma('pyretic.a.01.pyretic')], [Lemma('septic.a.01.septic')], [Lemma('synonymous.a.01.synonymous')], [Lemma('retrorse.a.01.retrorse')], [Lemma('periodic.a.01.periodic')], [Lemma('petalous.a.01.petalous')], [Lemma('anaphrodisiac.a.01.anaphrodisiac')], [Lemma('anaphrodisiac.a.01.anaphrodisiac')], [Lemma('placental.a.01.placental')], [Lemma('syncarpous.a.01.syncarpous')], [Lemma('eccrine.a.01.eccrine')], [Lemma('unapologetic.a.01.unapologetic')], [Lemma('undress.v.01.undress')], [Lemma('unappealable.a.01.unappealable')], [Lemma('repel.v.02.repel')], [Lemma('unappealing.a.02.unappealing')], [Lemma('unsympathetic.a.02.unsympathetic')], [Lemma('unappendaged.a.01.unappendaged')], [Lemma('unappetizing.a.01.unappetizing')], [Lemma('unappetizing.a.01.unappetizing')], [Lemma('defy.v.02.defy')], [Lemma('exempt.v.01.exempt')], [Lemma('theoretical.a.02.theoretical')], [Lemma('elective.a.01.elective')], [Lemma('elective.a.01.elective')], [Lemma('unopposable.a.01.unopposable')], [Lemma('depreciate.v.03.depreciate')], [Lemma('depreciate.v.02.depreciate')], [Lemma('unapproachable.a.01.unapproachable')], [Lemma('inappropriate.a.01.inappropriate')], [Lemma('disapprove.v.02.disapprove')], [Lemma('disapprove.v.01.disapprove')], [Lemma('disapproval.n.04.disapproval')], [Lemma('disapprove.v.02.disapprove')], [Lemma('disapprove.v.01.disapprove')], [Lemma('malapropos.a.01.malapropos')], [Lemma('peripteral.a.01.peripteral')], [Lemma('amphibious.a.02.amphibious'), Lemma('terrestrial.a.03.terrestrial')], [Lemma('igneous.a.02.igneous')], [Lemma('nonarbitrable.a.01.nonarbitrable')], [Lemma('nonarbitrary.a.01.nonarbitrary')], [Lemma('nonarboreal.a.01.nonarboreal')], [Lemma('nonarboreal.a.01.nonarboreal')], [Lemma('argillaceous.a.01.argillaceous')], [Lemma('arenaceous.a.01.arenaceous')], [Lemma('unargumentative.a.01.unargumentative')], [Lemma('disarm.v.01.disarm')], [Lemma('unarmed.a.01.unarmed')], [Lemma('armless.a.01.armless')], [Lemma('unarmed.a.02.unarmed')], [Lemma('armed.a.02.armed')], [Lemma('unarmored.a.02.unarmored')], [Lemma('unarmored.a.01.unarmored')], [Lemma('unarmored.a.01.unarmored')], [Lemma('unarmored.a.02.unarmored')], [Lemma('fall_asleep.v.01.fall_asleep')], [Lemma('sedate.v.01.sedate')], [Lemma('de-energize.v.01.de-energize')], [Lemma('de-energize.v.01.de-energise')], [Lemma('cause_to_sleep.v.01.cause_to_sleep')], [Lemma('disarrange.v.01.disarrange')], [Lemma('disarranged.a.01.disarranged')], [Lemma('subartesian.a.01.subartesian')], [Lemma('ingenuous.a.01.ingenuous')], [Lemma('artless.a.02.artless')], [Lemma('inarticulate.a.01.inarticulate')], [Lemma('unarticulated.a.01.unarticulated')], [Lemma('unarticulated.a.01.unarticulated')], [Lemma('natural.a.02.natural')], [Lemma('odd-toed_ungulate.n.01.odd-toed_ungulate')], [Lemma('disingenuous.a.01.disingenuous')], [Lemma('artful.a.02.artful')], [Lemma('descendant.n.01.descendant')], [Lemma('descendant.n.01.descendant')], [Lemma('descend.v.01.descend')], [Lemma('set.v.10.set')], [Lemma('descending.a.01.descending')], [Lemma('sexual.a.02.sexual')], [Lemma('unashamed.a.01.unashamed')], [Lemma('awake.a.01.awake')], [Lemma('interrogative.a.02.interrogative')], [Lemma('interrogative.a.02.interrogatory')], [Lemma('unassertive.a.01.unassertive')], [Lemma('unassigned.a.01.unassigned')], [Lemma('dissimilate.v.03.dissimilate')], [Lemma('dissimilate.v.02.dissimilate')], [Lemma('dissimilate.v.01.dissimilate')], [Lemma('unassisted.a.02.unassisted')], [Lemma('decouple.v.02.dissociate')], [Lemma('nonassociative.a.01.nonassociative')], [Lemma('nonassociative.a.01.nonassociative')], [Lemma('worry.v.03.worry')], [Lemma('worry.v.03.worry')], [Lemma('anastigmatic.a.02.anastigmatic')], [Lemma('stomatous.a.03.stomatous')], [Lemma('nonastringent.a.01.nonastringent')], [Lemma('symmetrical.a.01.symmetrical')], [Lemma('symmetrical.a.01.symmetrical')], [Lemma('synchronous.a.02.synchronous')], [Lemma('synchronous.a.01.synchronous')], [Lemma('syndetic.a.01.syndetic')], [Lemma('conventional.a.03.conventional')], [Lemma('holistic.a.01.holistic')], [Lemma('holistic.a.01.holistic')], [Lemma('tonal.a.02.tonal')], [Lemma('tonic.a.03.tonic')], [Lemma('toxic.a.01.toxic')], [Lemma('hypertrophied.a.01.hypertrophied')], [Lemma('detachable.a.01.detachable')], [Lemma('detach.v.01.detach')], [Lemma('detach.v.03.detach')], [Lemma('detached.a.04.detached')], [Lemma('unattached.a.02.unattached')], [Lemma('defend.v.02.defend')], [Lemma('miss.v.03.miss')], [Lemma('unaccompanied.a.02.unaccompanied')], [Lemma('inattentive.a.01.inattentive')], [Lemma('heedless.a.01.heedless')], [Lemma('dress_down.v.02.underdress')], [Lemma('dress_down.v.02.dress_down')], [Lemma('unattractive.a.01.unattractive')], [Lemma('repulsive.a.02.repulsive')], [Lemma('unattributable.a.01.unattributable')], [Lemma('predicative.a.01.predicative')], [Lemma('typical.a.01.typical')], [Lemma('inaudible.a.01.inaudible')], [Lemma('inauspicious.a.01.inauspicious')], [Lemma('unauthorized.a.01.unauthorized')], [Lemma('unauthorized.a.01.unauthorized')], [Lemma('allochthonous.a.01.allochthonous')], [Lemma('heteroecious.a.01.heteroecious')], [Lemma('endogamous.a.01.endogamous'), Lemma('exogamous.a.01.exogamous')], [Lemma('endogamous.a.01.endogamous'), Lemma('exogamous.a.01.exogamous')], [Lemma('heterogenous.a.02.heterogenous')], [Lemma('heterogenous.a.02.heterogenous')], [Lemma('heterologous.a.02.heterologous'), Lemma('homologous.a.02.homologous')], [Lemma('manual.a.02.manual')], [Lemma('heterotrophic.a.01.heterotrophic')], [Lemma('heterotrophic.a.01.heterotrophic')], [Lemma('summery.a.01.summery'), Lemma('vernal.a.02.vernal'), Lemma('wintry.a.01.wintry')], [Lemma('unavailable.a.01.unavailable')], [Lemma('vascular.a.01.vascular')], [Lemma('unavenged.a.01.unavenged')], [Lemma('inevitable.a.01.inevitable')], [Lemma('inevitable.a.01.inevitable')], [Lemma('virulent.a.02.virulent')], [Lemma('inevitable.a.01.inevitable')], [Lemma('disavow.v.01.disavow')], [Lemma('fall_asleep.v.01.fall_asleep')], [Lemma('asleep.a.01.asleep')], [Lemma('cause_to_sleep.v.01.cause_to_sleep')], [Lemma('fall_asleep.v.01.fall_asleep')], [Lemma('unawakened.a.01.unawakened')], [Lemma('unaware.a.01.unaware')], [Lemma('unmindful.a.01.unmindful')], [Lemma('home.a.01.home')], [Lemma('unawed.a.01.unawed')], [Lemma('unawed.a.01.unawed')], [Lemma('unawed.a.01.unawed')], [Lemma('nice.a.01.nice')], [Lemma('graceful.a.01.graceful')], [Lemma('awnless.a.01.awnless')], [Lemma('awned.a.01.awned')], [Lemma('awnless.a.01.awnless')], [Lemma('zonal.a.02.zonal')], [Lemma('front.n.04.front')], [Lemma('front.n.09.front')], [Lemma('advance.v.05.advance')], [Lemma('front.v.01.front')], [Lemma('veer.v.02.veer')], [Lemma('front.a.01.front')], [Lemma('forward.r.01.forward')], [Lemma('ahead.r.02.ahead')], [Lemma('ahead.r.02.forward')]]\n"
          ]
        }
      ]
    }
  ]
}